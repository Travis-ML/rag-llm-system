# Local RAG System with AI Agents

A complete self-hosted AI research platform running on Docker with GPU acceleration. Combines LLM inference, vector search, web search, and code execution - all running locally.

## ğŸ¯ What This Does

- **RAG (Retrieval-Augmented Generation)**: Query your PDF documents with AI
- **Web Search**: AI-powered web search with real-time results
- **Code Execution**: Run Python code through Jupyter integration
- **Multi-Modal Chat**: Single interface for all capabilities
- **100% Local**: Runs on your hardware with GPU acceleration

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Open WebUI                          â”‚
â”‚              (Single Interface for Everything)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â”‚                  â”‚                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
        â”‚   Ollama    â”‚    â”‚   SearXNG   â”‚   â”‚  Jupyter  â”‚
        â”‚ (LLM + GPU) â”‚    â”‚ (Web Search)â”‚   â”‚(Code Exec)â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Qdrant    â”‚   â”‚  RAG API    â”‚
        â”‚ (Vector DB) â”‚â”€â”€â”€â”‚  (Custom)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Components

| Service | Purpose | Port | GPU |
|---------|---------|------|-----|
| **Ollama** | LLM inference (llama3.2, nomic-embed-text) | 11434 | âœ… RTX 4070 |
| **Qdrant** | Vector database for embeddings | 6333/6334 | âŒ |
| **RAG API** | Custom RAG implementation (FastAPI) | 8000 | âŒ |
| **SearXNG** | Meta-search engine (privacy-focused) | 8080 | âŒ |
| **Open WebUI** | ChatGPT-like interface | 3000 | âŒ |
| **Jupyter** | Code execution environment | 8888 | âŒ |

## ğŸš€ Quick Start

### Prerequisites

- Docker Desktop with WSL2
- NVIDIA GPU with drivers installed
- NVIDIA Container Toolkit
- 16GB+ RAM recommended
- 20GB+ disk space

### Installation

1. **Clone this repository:**
```bash
git clone https://github.com/Travis-ML/rag-llm-system.git
cd rag-llm-system/rag-system
```

2. **Start all services:**
```bash
docker-compose up -d
```

3. **Pull required models:**
```bash
docker exec rag-ollama ollama pull llama3.2
docker exec rag-ollama ollama pull nomic-embed-text
```

4. **Access interfaces:**
- Open WebUI: http://localhost:3000
- Qdrant Dashboard: http://localhost:6334/dashboard
- SearXNG: http://localhost:8080
- Jupyter Lab: http://localhost:8888/lab?token=mysecrettoken123
- RAG API: http://localhost:8000

### First-Time Setup

**Configure Open WebUI:**

1. Create admin account on first visit
2. Go to Settings â†’ Models â†’ Set default to `llama3.2:latest`
3. Settings â†’ Web Search:
   - Enable Web Search âœ…
   - Search Engine: `searxng`
   - SearXNG URL: `http://searxng:8080/search?q=<query>`
4. Settings â†’ Code Execution:
   - Execution Engine: `jupyter`
   - Jupyter URL: `http://jupyter:8888`
   - Token: `mysecrettoken123`
5. Settings â†’ Documents:
   - Enable RAG âœ…
   - Embedding Model: `nomic-embed-text:latest`

## ğŸ“š Adding Documents

### Method 1: Upload via Open WebUI
1. Click the `+` icon
2. Upload PDFs directly
3. They auto-index to Qdrant

### Method 2: Bulk Processing
1. Copy PDFs to `./documents/` folder
2. Process them:
```bash
docker exec rag-application python process_pdfs.py
```

### Method 3: Via API
```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@/path/to/document.pdf"
```

## ğŸ’¬ Usage Examples

### Document Q&A
```
What does my RLHF paper say about reward models?
```

### Web Search + RAG
```
Compare recent developments in RLHF with what's in my documents
```

### Code Execution
```
Write Python code to analyze the first 100 Fibonacci numbers and plot them
```

### Multi-Step Research
```
1. Search the web for latest adversarial AI attacks
2. Check if my papers mention any of these attacks
3. Write code to simulate a simple prompt injection attack
```

## ğŸ› ï¸ Development

### Project Structure
```
rag-system/
â”œâ”€â”€ docker-compose.yml       # Main orchestration
â”œâ”€â”€ rag-app/                # Custom RAG API
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ rag_server.py       # FastAPI server
â”‚   â””â”€â”€ process_pdfs.py     # PDF indexing script
â”œâ”€â”€ documents/              # PDF storage
â””â”€â”€ README.md
```

### Updating Components

**Rebuild RAG API:**
```bash
docker-compose down rag-app
docker-compose build rag-app
docker-compose up -d rag-app
```

**Update models:**
```bash
docker exec rag-ollama ollama pull llama3.2:latest
```

**Reset vector database:**
```bash
docker-compose down -v qdrant
docker-compose up -d qdrant
```

## ğŸ”§ Troubleshooting

### Out of Memory
- Increase Docker Desktop memory: Settings â†’ Resources â†’ 16GB+
- Configure WSL2 memory in `~/.wslconfig`:
```ini
[wsl2]
memory=16GB
processors=8
```

### Container Exits with Code 137
- OOM killed - increase memory allocation
- Check: `docker stats`

### Can't Connect to Ollama
```bash
# Check if running
docker-compose ps

# Test connection
curl http://localhost:11434/api/tags
```

### Web Search Not Working
- Verify SearXNG URL in Open WebUI settings
- Test SearXNG directly: http://localhost:8080

### Code Execution Fails
- Check Jupyter token matches in settings
- Verify connection: http://localhost:8888

## ğŸ“Š Performance

**On RTX 4070 (12GB VRAM):**
- LLM Response Time: 2-5 seconds
- Vector Search: <100ms
- Document Indexing: ~30 chunks/second
- Code Execution: Near-instant

**Resource Usage:**
- Ollama: ~4GB VRAM (llama3.2)
- Qdrant: ~150MB RAM
- Open WebUI: ~300MB RAM
- Total: ~6GB RAM, 4GB VRAM

## ğŸ”’ Security Notes

- All services run locally - no data leaves your machine
- Change default tokens in production
- SearXNG provides privacy-focused web search
- Jupyter token should be changed in `docker-compose.yml`

## ğŸ“ Use Cases

- **AI Security Research**: Test adversarial attacks safely
- **Document Analysis**: Query research papers, reports, manuals
- **Research Assistant**: Web search + knowledge base + code execution
- **Learning**: Experiment with LLMs, RAG, vector databases
- **Privacy**: Keep all AI interactions local

## ğŸ“ Configuration

### Environment Variables

**Ollama:**
- `OLLAMA_HOST`: Default `http://rag-ollama:11434`

**RAG API:**
- `QDRANT_HOST`: Vector database host
- `QDRANT_PORT`: Default `6333`
- `OLLAMA_HOST`: LLM endpoint

**Jupyter:**
- `JUPYTER_TOKEN`: Authentication token
- `JUPYTER_ENABLE_LAB`: Enable JupyterLab interface

## ğŸ¤ Contributing

This is a personal research setup. Fork and modify as needed!

## ğŸ“„ License

MIT License - Use freely for research and learning

## ğŸ™ Acknowledgments

Built with:
- [Ollama](https://ollama.ai/) - Local LLM inference
- [Qdrant](https://qdrant.tech/) - Vector database
- [Open WebUI](https://github.com/open-webui/open-webui) - Chat interface
- [SearXNG](https://github.com/searxng/searxng) - Meta-search
- [LangChain](https://github.com/langchain-ai/langchain) - RAG framework
- [FastAPI](https://fastapi.tiangolo.com/) - API framework

## ğŸ”— Resources

- [Documentation](docs/) - Detailed guides (coming soon)
- [Examples](examples/) - Usage examples (coming soon)
- [Troubleshooting](TROUBLESHOOTING.md) - Common issues (coming soon)

---

**Built for AI Security Research** | **100% Local & Private** | **GPU Accelerated**
